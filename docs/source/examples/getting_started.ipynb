{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\nGetting started\n===============\n\nThis tutorial highlights the basics of the\nhigh-level API for ensemble classes, the model selection suite and\nfeatures visualization.\n\n============================  =================================================\n                   Tutorials                                            Content\n============================  =================================================\n`ensemble-guide`         How to build, fit and predict with an ensemble\n`model-selection-guide`  How to compare several estimators in one go\n`visualization-guide`    Plotting functionality\n============================  =================================================\n\nThe `advanced high-level API tutorials <ensemble-tutorial>` shows how to\nleverage advanced features such as probabilistic layers, feature propagation\netc. For tutorials on low-level mechanics, see\n`the mechanics guides <learner_tutorial>`.\n\n\nPreliminaries\n-------------\n\nWe use the following setup throughout:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom pandas import DataFrame\nfrom sklearn.metrics import f1_score\nfrom sklearn.datasets import load_iris\n\nseed = 2017\nnp.random.seed(seed)\n\ndef f1(y, p): return f1_score(y, p, average='micro')\n\ndata = load_iris()\nidx = np.random.permutation(150)\nX = data.data[idx]\ny = data.target[idx]\nprint(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nEnsemble guide\n--------------\nBuilding an ensemble\n^^^^^^^^^^^^^^^^^^^^\nInstantiating a fully specified ensemble is straightforward and requires\nthree steps: first create the instance, second add the intermediate layers, and\nfinally the meta estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.ensemble import SuperLearner\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n# --- Build ---\n# Passing a scoring function will create cv scores during fitting\n# the scorer should be a simple function accepting to vectors and returning a scalar\n\nensemble = SuperLearner(scorer=f1, random_state=seed, verbose=1)\n\n# Build the first layer\nensemble.add([RandomForestClassifier(random_state=seed), SVC()])\n\n# Attach the final meta estimator\nensemble.add_meta(LogisticRegression())\n\n# --- Use ---\n\n# Fit ensemble\nensemble.fit(X[:75], y[:75])\n\n# Predict\npreds = ensemble.predict(X[75:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To check the performance of estimator in the layers, call the ``data``\nattribute. The attribute can be wrapped in a :class:`pandas.DataFrame`,\nbut prints in a tabular format as is.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(ensemble.data)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}