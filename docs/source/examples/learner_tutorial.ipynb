{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n\n.. currentmodule: mlens.parallel.learner\n\nLearner Mechanics\n=================\n\nML-Ensemble is designed to provide an easy user interface. But it is also designed\nto be extremely flexible, all the wile providing maximum concurrency at minimal\nmemory consumption. The lower-level API that builds the ensemble and manages the\ncomputations is constructed in as modular a fashion as possible.\n\nThe low-level API introduces a computational graph-like environment that you can\ndirectly exploit to gain further control over your ensemble. In fact, building\nyour ensemble through the low-level API is almost as straight forward as using the\nhigh-level API. In this tutorial, we will walk through the key core :class:`Learner` class.\n\n\nThe Learner API\n^^^^^^^^^^^^^^^\n\nBasics\n------\n\nWhen you pass an estimator to an ensemble, it gets wrapper\nin a :class:`Learner` instance. This class records relevant information\nabout the estimator and manages the cross-validated fit. It also keeps\ntrack of which preprocessing pipeline to use (if any). You can think of a learner as a core node in a computational graph. This node has as auxiliary nodes a blueprint estimator to use for estimation, an indexer, a pointer to a preprocessing node, and an output mapping:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.utils.dummy import OLS\nfrom mlens.parallel import Learner\nfrom mlens.index import FoldIndex\n\n\nindexer = FoldIndex(n_splits=2)            # Define a training strategy\nlearner = Learner(estimator=OLS(),         # Declare estimator\n                  preprocess=None,         # We'll get to this\n                  indexer=indexer,         # Our above instance\n                  name='ols',              # Don't reuse name\n                  attr='predict',          # Attribute for prediction\n                  scorer=None,             # To get cv scores\n                  output_columns={0: 0},   # Prediction feature index\n                  verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. currentmodule: mlens.index\n\nThe ``name`` gives the learner a cache reference. When the learner is\nconstructed by the high-level API , the name is guaranteed to be unique, when\nyou use the low-level API this is your responsibility. The ``output_columns``\ntells the learner which column index in an output array it should populate\nwhen predicting: ``attr`` tells the learner which method to use.\nThe output_columns can contain several entries if your indexer creates\npartitions (see :class:`SubsetIndex` and :class:`ClusteredSubsetIndex`).\n\n.. currentmodule: mlens.parallel.learner\n\nThe learner doesn't do any heavy lifting itself, it manages the creation\nof auxiliary :class:`SubLearner` nodes for each fold during estimation.\nThis process is dynamic: the sub-learner is a temporary instance that\nperform the estimation asked of it and caches any output. So to fit\na learner, we first fit the indexer, then iterate through each of the\nsub-learners created for the task:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os, tempfile\nimport numpy as np\n\nX = np.arange(20).reshape(10, 2)\ny = np.random.rand(10)\n\n# Fit the indexer to data to create fold indexes\nindexer.fit(X)\n\n# Specify a cache directory\npath = tempfile.TemporaryDirectory(dir=os.getcwd())\n\n# Declare which type of job (fit, predict, transform)\nfor sub_learner in learner('fit', X, y):\n    sub_learner('fit', path.name)\n\nprint(\"Cached items:\\n%r\" % os.listdir(path.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting the learner puts three copies of the OLS estimator in the ``path``\ndirectory: one for each fold and one for the full dataset.\nThese are named as ``[name]__[col_id]__[fold_id]``. To load these into the\nlearner, call ``collect``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "learner.collect(path.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main estimator, fitted on all data, gets stored into the\n``fitted_learner`` attribute, while the others are stored in the\n``fitted_sublearners``. These attributes are generators that will\niterate over each fitted estimator and yield a deep copy of them.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So to generate predictions, we can either use the ``fitted_sublearners``\ngenerator create cross-validated predictions, or ``fitted_learner``\ngenerator to generate predictions for the whole input set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But to generate predictions, the learner needs an output array to populate.\nIn particular, the learner will populate the columns given in the\n``output_columns`` parameter. Here, we use the ``transform`` task, which\nuses the ``fitted_sublearners`` generator to produce cross-validated\npredictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "P = np.zeros((y.shape[0], 1))\nfor sub_learner in learner('transform', X, P):\n    sub_learner('transform', path.name)\n    print('P:')\n    print(P)\n    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above loop, a sub-segment of ``P`` is updated by each sublearner\nspawned by the learner. To instead produce predictions for the full\ndataset using the final estimator, task the learner to ``predict``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ML-Ensemble follows the Scikit-learn API, so if you wish to update any\nhyper-parameters of the estimator, use the ``get_params`` and ``set_params``\nAPI:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Params before:\")\nprint(learner.get_params())\n\nlearner.set_params(estimator__offset=1, indexer__n_splits=3)\n\nprint(\"Params after:\")\nprint(learner.get_params())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Updating the indexer on one learner updates the indexer on all</p></div>\n learners that where initiated with the same instance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Partitioning\n------------\n\nWe can create several other types of learners by\nvarying the estimation strategy. An especially interesting strategy is to\npartition the training set and create several learners fitted on a given\npartition. This will create one prediction feature per partition\nSo we now need to specify in the ``output_columns`` dict which partition\nis given which column in the output array.\nHere, we fit the OLS model using two partitions and two fold CV on each\npartition. Note that by passing the output array to the sub-learner\nduring fitting, we get predictions immediately.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.index import SubsetIndex\n\nindexer = SubsetIndex(n_partitions=2, n_splits=2, X=X)\nlearner = Learner(estimator=OLS(),\n                  preprocess=None,\n                  indexer=indexer,\n                  name='ols',\n                  attr='predict',\n                  scorer=None,\n                  output_columns={0: 0, 1: 1},\n                  verbose=True)\n\n# P needs 2 cols\nP = np.zeros((y.shape[0], 2))\n\n# Pass P during 'fit' to get prediction immediately\nfor sub_learner in learner('fit', X, y, P):\n    sub_learner.fit(path.name)\n    print('P:')\n    print(P)\n    print()\n\nlearner.collect(path.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each sub-learner records fit and predict times during fitting, and if\na scorer is passed scores the predictions as well. The learner aggregates\nthis data into a ``raw_data`` list, and a tabular ``data`` attribute:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Data:\\n %s\" % learner.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing\n-------------\n\nIn general, several estimators share the same preprocessing pipeline,\nso we don't want\nto pass the object itself along, or we risk conflicts. Instead,\nthe learner is given a pointer to the caches preprocessing pipeline so that\nit can load when needed. To facilitate preprocessing across several learners,\nwe need new type of node, the :class:``Transformer``. This class behaves\nsimilarly to the learner, but differs in that it doesn't output any\npredictions or transformations, but merely fits and caches the preprocessing\npipelines. The primary reason for this design is that the transformer would\nneed to a transformed copy of the input data for each fold, which would\nquickly result in massive memory consumption.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So to construct a learner with preprocessing, we begin by constructing the\ntransformer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mlens.utils.dummy import Scale\nfrom mlens.parallel import Transformer\n\ntransformer = Transformer(pipeline=[('trans', Scale())],\n                          indexer=indexer,\n                          name='sc',\n                          verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, to build the learner we now pass the ``name`` of the transformer as\nthe ``preprocess`` argument to the learner. Here', we'll also include a\nscoring function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mse(y, p): return np.mean((y - p) ** 2)\n\nlearner = Learner(estimator=OLS(),\n                  preprocess='sc',\n                  indexer=indexer,\n                  name='ols',\n                  attr='predict',\n                  scorer=mse,\n                  output_columns={0: 0, 1: 1},\n                  verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To fit the learner, we must first fit the transformer. Both follow the\nsame API, so we simply repeat the above step for each instance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "P = np.zeros((y.shape[0], 2))\n\nfor st in transformer('fit', X, y):\n    st('fit', path.name)\n\nfor lr in learner('fit', X, y, P):\n    lr('fit', path.name)\n\ntransformer.collect(path.name)\nlearner.collect(path.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the cache now contains the transfomers as well:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Cache: %r\" % os.listdir(path.name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimation Data\n---------------\n\nWhen fitting the learner, data is collected and stored on a case, estimator\nand partition basis. Standard data is fit time (``ft``), predict time (``pr``)\nand if applicable, test set prediction scores. Since we use cross-validated\nestimation, we get mean (``-m``) and standard deviation (``-s``) for free.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Data:\\n%s\" % learner.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data is stored as a ``dict`` that prints in tabular\nformat for readability. You can however also pass the ``data`` attribute\nto a :class:`pandas.DataFrame` if you wish.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we handle several learners by grouping them in a layer in the\n`layer mechanics tutorial <layer_tutorial>`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}