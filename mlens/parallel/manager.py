"""ML-Ensemble

:author: Sebastian Flennerhag
:copyright: 2017
:licence: MIT

Parallel processing job managers.
"""
# pylint: disable=too-few-public-methods
# pylint: disable=too-many-arguments
# pylint: disable=useless-super-delegation

from __future__ import with_statement, division

import gc
import os
import shutil
import subprocess
import tempfile
import warnings

from abc import ABCMeta, abstractmethod

import numpy as np
from scipy.sparse import issparse, hstack

from .. import config
from ..externals.joblib import Parallel, dump, load
from ..externals.sklearn.validation import check_random_state
from ..utils import check_initialized
from ..utils.exceptions import (ParallelProcessingError,
                                ParallelProcessingWarning)

ENGINES = list()


###############################################################################
def dump_array(array, name, path):
    """Dump array for memmapping."""
    # First check if the array is on file
    if isinstance(array, str):
        # Load file from disk. Need to dump if not memmaped already
        if not array.split('.')[-1] in ['mmap', 'npy', 'npz']:
            # Try loading the file assuming a csv-like format
            array = _load(array)

    if isinstance(array, str):
        # If arr remains a string, it's pointing to an mmap file
        f = array
    else:
        # Dump ndarray on disk
        f = os.path.join(path, '%s.mmap' % name)
        if os.path.exists(f):
            os.unlink(f)
        dump(array, f)
    return f


def _load(arr):
    """Load array from file using default settings."""
    if arr.split('.')[-1] in ['npy', 'npz']:
        return np.load(arr)
    else:
        try:
            return np.genfromtxt(arr)
        except Exception as e:
            raise IOError("Could not load X from %s, does not "
                          "appear to be a valid ndarray. "
                          "Details:\n%r" % (arr, e))


def _load_mmap(f):
    """Load a mmap presumably dumped by joblib, otherwise try numpy."""
    try:
        return load(f, mmap_mode='r')
    except (IndexError, KeyError):
        # Joblib's 'load' func fails on npy and npz: use numpy.load
        return np.load(f, mmap_mode='r')


###############################################################################
class Job(object):

    """Container class for holding job data.

    See Also
    --------
    :class:`ParallelProcessing`, :class:`ParallelEvaluation`
    """

    __slots__ = ['y', 'predict_in', 'predict_out', 'dir', 'job', 'tmp',
                 '_n_dir']

    def __init__(self, job):
        self.job = job
        self.y = None
        self.predict_in = None
        self.predict_out = None
        self.tmp = None
        self.dir = None
        self._n_dir = 0

    def update(self):
        """Shift output array to input array.

        Parameters
        ----------
        shuffle : bool (default = False)
            whether to shuffle the new input data.

        random_state : int, optional
            random seed to use.
        """
        # Enforce csr on spare matrices
        if issparse(self.predict_out) and not \
                self.predict_out.__class__.__name__.startswith('csr'):
            self.predict_out = self.predict_out.tocsr()

        self.predict_in = self.predict_out
        self.rebase()

    def rebase(self):
        """Rebase output labels to input indexing."""
        if self.y is not None and (self.y.shape[0] > self.predict_in.shape[0]):
            # This is legal if X is a prediction matrix generated by predicting
            # only a subset of the original training set.
            # Since indexing is strictly monotonic, we can simply discard
            # the first observations in y to get the corresponding labels.
            rebase = self.y.shape[0] - self.predict_in.shape[0]
            self.y = self.y[rebase:]

    def shuffle(self, shuffle, random_state):
        """Shuffle inputs if asked."""
        if shuffle:
            r = check_random_state(random_state)
            idx = r.permutation(self.y.shape[0])
            self.predict_in = self.predict_in[idx]
            self.y = self.y[idx]

    def subdir(self, name=None):
        """Create new subdirectory in dir"""
        if not name:
            name = str(self._n_dir)
            self._n_dir += 1

        path = os.path.join(self.dir, ".mlens_subdir_%s" % name)
        if os.path.exists(path):
            raise OSError("Subdirectory exist. Clear estimation cache.")
        os.mkdir(path)
        return path

    @property
    def args(self):
        """Produce args dict"""
        trans_feed = {'X': self.predict_in}
        learn_feed = {'X': self.predict_in,
                      'P': self.predict_out}

        if self.job in ['fit', 'evaluate']:
            trans_feed['y'] = self.y
            learn_feed['y'] = self.y

        out = {'transformer': trans_feed,
               'learner': learn_feed,
               'dir': self.subdir(),
               'job': self.job}

        return out


###############################################################################
class BaseProcessor(object):

    """Parallel processing base class.

    Base class for parallel processing engines.
    """

    __meta_class__ = ABCMeta

    __slots__ = ['caller', '__initialized__', '__threading__', 'job']

    @abstractmethod
    def __init__(self, caller):
        self.job = None
        self.caller = caller
        self.__initialized__ = 0
        self.__threading__ = self.caller.backend == 'threading'

    def __enter__(self):
        return self

    def _initialize(self, job, X, y=None, path=None):
        """Create a job instance for estimation."""
        job = Job(job)

        if path is None:
            path = config.TMPDIR
        try:
            job.tmp = tempfile.TemporaryDirectory(
                prefix=config.PREFIX, dir=path)
            job.dir = job.tmp.name
        except AttributeError:
            # Fails on python 2
            job.dir = tempfile.mkdtemp(prefix=config.PREFIX, dir=path)

        # --- Prepare inputs
        for name, arr in zip(('X', 'y'), (X, y)):
            if arr is None:
                continue

            # Dump data in cache
            if self.__threading__:
                # No need to memmap
                f = None
                if isinstance(arr, str):
                    arr = _load(arr)
            else:
                f = dump_array(arr, name, job.dir)

            # Store data for processing
            if name == 'y' and arr is not None:
                job.y = arr if self.__threading__ else _load_mmap(f)
            elif name == 'X':
                job.predict_in = arr \
                    if self.__threading__ else _load_mmap(f)

        self.job = job
        self.__initialized__ = 1
        gc.collect()
        return self

    def __exit__(self, *args):
        """Ensure cache cleanup"""
        if getattr(self, 'job', None) is not None:
            # Delete all contents from cache
            try:
                self.job.tmp.cleanup()

            except (AttributeError, OSError):
                # Fall back on shutil for python 2, can also fail on windows
                try:
                    shutil.rmtree(self.job.dir)
                except OSError:
                    # Can fail on windows, need to use the shell
                    try:
                        subprocess.Popen('rmdir /S /Q %s' % self.job.dir,
                                         shell=True, stdout=subprocess.PIPE,
                                         stderr=subprocess.PIPE)
                    except OSError:
                        warnings.warn(
                            "Failed to delete cache at %s."
                            "If created with default settings, will be "
                            "removed on reboot. For immediate "
                            "removal, manual removal is required." %
                            self.job.dir, ParallelProcessingWarning)

            finally:
                # Always release process memory
                del self.job
                gc.collect()

                if gc.garbage:
                    warnings.warn("Clearing process memory failed, "
                                  "uncollected:\n%r." % gc.garbage,
                                  ParallelProcessingWarning)

                self.__initialized__ = 0


class ParallelProcessing(BaseProcessor):

    """Parallel processing engine.

    Engine for running ensemble estimation.

    Parameters
    ----------
    caller :  obj
        the caller of the job. Either a Layer or a meta layer class
        such as Sequential.
    """
    def __init__(self, caller):
        super(ParallelProcessing, self).__init__(caller)

    def process(self, job, X, y=None, path=None, return_preds=False):
        """Fit all layers in the attached :class:`Sequential`."""
        self._initialize(job=job, X=X, y=y, path=path)
        check_initialized(self)

        # Process each layer sequentially with the same worker pool
        with Parallel(n_jobs=self.caller.n_jobs,
                      temp_folder=self.job.dir,
                      max_nbytes=None,
                      mmap_mode='w+',
                      verbose=self.caller.verbose,
                      backend=self.caller.backend) as parallel:

            for lyr in self.caller:
                # Shuffle inputs during training if required
                if self.job.job == 'fit':
                    self.job.shuffle(lyr.shuffle, lyr.random_state)

                # Process layer
                self._partial_process(lyr, parallel)

                # Update input array with output array
                self.job.update()

        if return_preds:
            return self.get_preds(lyr.dtype)

    def _partial_process(self, layer, parallel):
        """Generate prediction matrix for a given :class:`layer`."""
        layer.indexer.fit(self.job.predict_in, self.job.y, self.job.job)
        self._gen_prediction_array(layer, self.__threading__)

        # Run estimation to populate prediction matrix
        layer.set_output_columns(self.job.y)
        layer(parallel, self.job.args)

        # Propagate features from input to output
        if layer.n_feature_prop:
            self._propagate_features(layer)

    def _propagate_features(self, lyr):
        """Propagate features from input array to output array."""
        p_out, p_in = self.job.predict_out, self.job.predict_in

        # Check for loss of obs between layers (i.e. blend)
        n_in, n_out = p_in.shape[0], p_out.shape[0]
        r = int(n_in - n_out)

        # Propagate features as the n first features of the outgoing array
        if not issparse(p_in):
            # Simple item setting
            p_out[:, :lyr.n_feature_prop] = p_in[r:, lyr.propagate_features]
        else:
            # Need to populate propagated features using scipy sparse hstack
            self.job.predict_out = hstack([p_in[r:, lyr.propagate_features],
                                           p_out[:, lyr.n_feature_prop:]]
                                          ).tolil()

    def _gen_prediction_array(self, lyr, threading):
        """Generate prediction array either in-memory or persist to disk."""
        shape = self._get_lyr_sample_size(lyr)
        if threading:
            self.job.predict_out = np.empty(shape, dtype=lyr.dtype)
        else:
            f = os.path.join(self.job.dir, '%s.mmap' % lyr.name)
            try:
                self.job.predict_out = np.memmap(filename=f,
                                                 dtype=lyr.dtype,
                                                 mode='w+',
                                                 shape=shape)
            except Exception as exc:
                raise OSError("Cannot create prediction matrix of shape ("
                              "%i, %i), size %i MBs, for %s.\n Details:\n%r" %
                              (shape[0], shape[1],
                               8 * shape[0] * shape[1] / (1024 ** 2),
                               lyr.name, exc))

    def _get_lyr_sample_size(self, lyr):
        """Decide what sample size to create P with based on the job type."""
        s0 = lyr.indexer.n_test_samples if self.job.job != 'predict' else \
            lyr.indexer.n_samples

        # Number of prediction columns depends on:
        # 1. number of estimators in layer
        # 2. if ``predict_proba``, number of classes in training set
        # 3. number of subsets (default is one for all data)
        # 4. number of features to propagate
        # Note that 1., 3. and 4. are params but 2. is data dependent
        s1 = lyr.n_pred

        if lyr.proba:
            if self.job.job == 'fit':
                lyr.set_output_columns(self.job.y)

            s1 *= lyr.classes_

        if lyr.propagate_features is not None:
            s1 += lyr.n_feature_prop

        return s0, s1

    def get_preds(self, dtype=None, order='C'):
        """Return prediction matrix.

        Parameters
        ----------
        dtype : numpy dtype object, optional
            data type to return

        order : str (default = 'C')
            data order. See :class:`numpy.asarray` for details.
        """
        if not hasattr(self, 'job'):
            raise ParallelProcessingError("Processor has been terminated: "
                                          "cannot retrieve final prediction "
                                          "array from cache.")
        if dtype is None:
            dtype = self.caller.dtype

        if issparse(self.job.predict_out):
            return self.job.predict_out
        return np.asarray(self.job.predict_out, dtype=dtype, order=order)


###############################################################################
class ParallelEvaluation(BaseProcessor):

    """Parallel cross-validation engine.

    Parameters
    ----------
    caller : :class:`Evaluator`
        The ``Evaluator`` that instantiated the processor.
    """

    def __init__(self, caller):
        super(ParallelEvaluation, self).__init__(caller)

    def _process(self, attr, X, y, path=None):
        """Fit all layers in the attached :class:`Sequential`."""
        self._initialize(job='evaluate', X=X, y=y, path=path)
        check_initialized(self)

        # Use context manager to ensure same parallel job during entire process
        with Parallel(n_jobs=self.caller.n_jobs,
                      temp_folder=self.job.dir,
                      max_nbytes=None,
                      mmap_mode='w+',
                      verbose=self.caller.verbose,
                      backend=self.caller.backend) as parallel:

            f = self.caller.__engine__(self.caller)

            getattr(f, attr)(parallel,
                             self.job.predict_in,
                             self.job.y,
                             self.job.dir)

    def process(self, case, X, y, path=None):
        """Fit estimators"""
        self._initialize(job='fit', X=X, y=y, path=path)
        check_initialized(self)

        # Use context manager to ensure same parallel job during entire process
        with Parallel(n_jobs=self.caller.n_jobs,
                      temp_folder=self.job.dir,
                      max_nbytes=None,
                      mmap_mode='w+',
                      verbose=self.caller.verbose,
                      backend=self.caller.backend) as parallel:

            self.caller.indexer.fit(
                self.job.predict_in, self.job.y, self.job.job)

            self.caller(parallel, self.job.args, case)
