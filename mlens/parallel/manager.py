"""ML-Ensemble

:author: Sebastian Flennerhag
:copyright: 2017
:licence: MIT

Parallel processing job managers.
"""
# pylint: disable=too-few-public-methods
# pylint: disable=too-many-arguments
# pylint: disable=too-many-instance-attributes
# pylint: disable=useless-super-delegation

from __future__ import with_statement, division

import gc
import os
import shutil
import subprocess
import tempfile
import warnings

from abc import ABCMeta, abstractmethod

import numpy as np
from scipy.sparse import issparse, hstack

from .. import config
from ..externals.joblib import Parallel, dump, load
from ..utils import check_initialized
from ..utils.exceptions import (ParallelProcessingError,
                                ParallelProcessingWarning)
from ..externals.sklearn.validation import check_random_state


###############################################################################
def dump_array(array, name, path):
    """Dump array for memmapping."""
    # First check if the array is on file
    if isinstance(array, str):
        # Load file from disk. Need to dump if not memmaped already
        if not array.split('.')[-1] in ['mmap', 'npy', 'npz']:
            # Try loading the file assuming a csv-like format
            array = _load(array)

    if isinstance(array, str):
        # If arr remains a string, it's pointing to an mmap file
        f = array
    else:
        # Dump ndarray on disk
        f = os.path.join(path, '%s.mmap' % name)
        if os.path.exists(f):
            os.unlink(f)
        dump(array, f)
    return f


def _load(arr):
    """Load array from file using default settings."""
    if arr.split('.')[-1] in ['npy', 'npz']:
        return np.load(arr)
    else:
        try:
            return np.genfromtxt(arr)
        except Exception as e:
            raise IOError("Could not load X from %s, does not "
                          "appear to be a valid ndarray. "
                          "Details:\n%r" % (arr, e))


def _load_mmap(f):
    """Load a mmap presumably dumped by joblib, otherwise try numpy."""
    try:
        return load(f, mmap_mode='r')
    except (IndexError, KeyError):
        # Joblib's 'load' func fails on npy and npz: use numpy.load
        return np.load(f, mmap_mode='r')


###############################################################################
class Job(object):

    """Container class for holding job data.

    See Also
    --------
    :class:`ParallelProcessing`, :class:`ParallelEvaluation`
    """

    __slots__ = ['y', 'predict_in', 'predict_out', 'dir', 'job', 'tmp',
                 '_n_dir', 'kwargs']

    def __init__(self, job, **kwargs):
        self.job = job
        self.kwargs = kwargs
        self.y = None
        self.predict_in = None
        self.predict_out = None
        self.tmp = None
        self.dir = None
        self._n_dir = 0

    def clear(self):
        """Clear output data for new task"""
        self.predict_out = None

    def update(self):
        """Shift output array to input array.

        Parameters
        ----------
        shuffle : bool (default = False)
            whether to shuffle the new input data.

        random_state : int, optional
            random seed to use.
        """
        if self.predict_out is None:
            return
        # Enforce csr on spare matrices
        if issparse(self.predict_out) and not \
                self.predict_out.__class__.__name__.startswith('csr'):
            self.predict_out = self.predict_out.tocsr()

        self.predict_in = self.predict_out
        self.rebase()

    def rebase(self):
        """Rebase output labels to input indexing."""
        if self.y is not None and (self.y.shape[0] > self.predict_in.shape[0]):
            # This is legal if X is a prediction matrix generated by predicting
            # only a subset of the original training set.
            # Since indexing is strictly monotonic, we can simply discard
            # the first observations in y to get the corresponding labels.
            rebase = self.y.shape[0] - self.predict_in.shape[0]
            self.y = self.y[rebase:]

    def shuffle(self, shuffle, random_state):
        """Shuffle inputs if asked."""
        if shuffle:
            r = check_random_state(random_state)
            idx = r.permutation(self.y.shape[0])
            self.predict_in = self.predict_in[idx]
            self.y = self.y[idx]

    def subdir(self, name=None):
        """Create new subdirectory in dir"""
        if not name:
            name = str(self._n_dir)
            self._n_dir += 1

        path = os.path.join(self.dir, "task_%s" % name)
        if os.path.exists(path):
            raise OSError("Subdirectory exist. Clear estimation cache.")
        os.mkdir(path)
        return path

    @property
    def args(self):
        """Produce args dict"""
        aux_feed = {'X': self.predict_in, 'P': None}
        est_feed = {'X': self.predict_in, 'P': self.predict_out}

        if self.job in ['fit', 'evaluate']:
            est_feed['y'] = self.y
            aux_feed['y'] = self.y

        if self.kwargs:
            est_feed.update(self.kwargs)
            aux_feed.update(self.kwargs)

        out = {'auxiliary': aux_feed,
               'estimator': est_feed,
               'dir': self.subdir(),
               'job': self.job}

        return out


###############################################################################
class BaseProcessor(object):

    """Parallel processing base class.

    Base class for parallel processing engines.
    """

    __meta_class__ = ABCMeta

    __slots__ = ['caller', '__initialized__', '__threading__', 'job',
                 'n_jobs', 'backend', 'verbose']

    @abstractmethod
    def __init__(self, backend=None, n_jobs=None, verbose=None):
        self.job = None
        self.__initialized__ = 0

        self.backend = config.BACKEND if not backend else backend
        self.n_jobs = -1 if not n_jobs else n_jobs
        self.verbose = False if not verbose else verbose
        self.__threading__ = self.backend == 'threading'

    def __enter__(self):
        return self

    def _initialize(self, job, X, y=None, path=None, **kwargs):
        """Create a job instance for estimation."""
        job = Job(job, **kwargs)

        if path is None:
            path = config.TMPDIR
        try:
            job.tmp = tempfile.TemporaryDirectory(
                prefix=config.PREFIX, dir=path)
            job.dir = job.tmp.name
        except AttributeError:
            # Fails on python 2
            job.dir = tempfile.mkdtemp(prefix=config.PREFIX, dir=path)

        # --- Prepare inputs
        for name, arr in zip(('X', 'y'), (X, y)):
            if arr is None:
                continue

            # Dump data in cache
            if self.__threading__:
                # No need to memmap
                f = None
                if isinstance(arr, str):
                    arr = _load(arr)
            else:
                f = dump_array(arr, name, job.dir)

            # Store data for processing
            if name == 'y' and arr is not None:
                job.y = arr if self.__threading__ else _load_mmap(f)
            elif name == 'X':
                job.predict_in = arr \
                    if self.__threading__ else _load_mmap(f)

        self.job = job
        self.__initialized__ = 1
        gc.collect()
        return self

    def __exit__(self, *args):
        """Ensure cache cleanup"""
        if getattr(self, 'job', None) is not None:
            # Delete all contents from cache
            try:
                self.job.tmp.cleanup()

            except (AttributeError, OSError):
                # Fall back on shutil for python 2, can also fail on windows
                try:
                    shutil.rmtree(self.job.dir)
                except OSError:
                    # Can fail on windows, need to use the shell
                    try:
                        subprocess.Popen('rmdir /S /Q %s' % self.job.dir,
                                         shell=True, stdout=subprocess.PIPE,
                                         stderr=subprocess.PIPE)
                    except OSError:
                        warnings.warn(
                            "Failed to delete cache at %s."
                            "If created with default settings, will be "
                            "removed on reboot. For immediate "
                            "removal, manual removal is required." %
                            self.job.dir, ParallelProcessingWarning)

            finally:
                # Always release process memory
                del self.job
                gc.collect()

                if gc.garbage:
                    warnings.warn("Clearing process memory failed, "
                                  "uncollected:\n%r." % gc.garbage,
                                  ParallelProcessingWarning)

                self.__initialized__ = 0


class ParallelProcessing(BaseProcessor):

    """Parallel processing engine.

    Engine for running ensemble estimation.

    Parameters
    ----------
    caller :  obj
        the caller of the job. Either a Layer or a meta layer class
        such as Sequential.
    """
    def __init__(self, *args, **kwargs):
        super(ParallelProcessing, self).__init__(*args, **kwargs)

    def process(self, caller, job, X, y=None, **kwargs):
        """Process job."""
        path = kwargs.pop('path', None)
        r = kwargs.pop('return_preds', False)
        out = None if not r else list()
        return_names = list() if isinstance(r, bool) else r

        self._initialize(job=job, X=X, y=y, path=path, **kwargs)
        check_initialized(self)

        with Parallel(n_jobs=self.n_jobs,
                      temp_folder=self.job.dir,
                      max_nbytes=None,
                      mmap_mode='w+',
                      verbose=self.verbose,
                      backend=self.backend) as parallel:

            for task in caller:
                self.job.clear()

                self._partial_process(task, parallel)

                if task.name in return_names:
                    out.append(self.get_preds(getattr(task, 'dtype', None)))

                self.job.update()

        if r and not out:
            out = self.get_preds(dtype=getattr(task, 'dtype', None))
        return out

    def _partial_process(self, task, parallel):
        """Process given task"""
        # Shuffle data if required
        if self.job.job == 'fit':
            self.job.shuffle(getattr(task, 'shuffle', False),
                             getattr(task, 'random_state', None))

        # Prep task
        task.indexer.fit(self.job.predict_in, self.job.y, self.job.job)
        if not getattr(task, '__no_output__', False):
            task.set_output_columns(self.job.predict_in, self.job.y)
            self._gen_prediction_array(task, self.__threading__)

        # Run estimation to populate prediction matrix
        task(parallel, self.job.args)

        # Propagate features from input to output
        if getattr(task, 'n_feature_prop', False):
            self._propagate_features(task)

    def _propagate_features(self, task):
        """Propagate features from input array to output array."""
        p_out, p_in = self.job.predict_out, self.job.predict_in

        # Check for loss of obs between layers (i.e. with blendindex)
        n_in, n_out = p_in.shape[0], p_out.shape[0]
        r = int(n_in - n_out)

        # Propagate features as the n first features of the outgoing array
        if not issparse(p_in):
            # Simple item setting
            p_out[:, :task.n_feature_prop] = p_in[r:, task.propagate_features]
        else:
            # Need to populate propagated features using scipy sparse hstack
            self.job.predict_out = hstack([p_in[r:, task.propagate_features],
                                           p_out[:, task.n_feature_prop:]]
                                          ).tolil()

    def _gen_prediction_array(self, task, threading):
        """Generate prediction array either in-memory or persist to disk."""
        shape = self._get_array_size(task)
        if threading:
            self.job.predict_out = np.empty(
                shape, dtype=getattr(task, 'dtype', config.DTYPE))
        else:
            f = os.path.join(self.job.dir, '%s_out_array.mmap' % task.name)
            try:
                self.job.predict_out = np.memmap(filename=f,
                                                 dtype=task.dtype,
                                                 mode='w+',
                                                 shape=shape)
            except Exception as exc:
                raise OSError("Cannot create prediction matrix of shape ("
                              "%i, %i), size %i MBs, for %s.\n Details:\n%r" %
                              (shape[0], shape[1],
                               8 * shape[0] * shape[1] / (1024 ** 2),
                               task.name, exc))

    def _get_array_size(self, task):
        """Decide what size to create P with based on the job type."""
        s0 = task.indexer.n_test_samples if self.job.job != 'predict' else \
            task.indexer.n_samples

        # Number of prediction columns depends on:
        # 1. number of estimators in layer
        # 2. if ``predict_proba``, number of classes in training set
        # 3. number of subsets (default is one for all data)
        # 4. number of features to propagate
        # Note that 1., 3. and 4. are params but 2. is data dependent
        s1 = task.n_pred

        if getattr(task, 'proba', False):
            s1 *= task.classes_

        if getattr(task, 'propagate_features', None) is not None:
            s1 += task.n_feature_prop

        return s0, s1

    def get_preds(self, dtype=None, order='C'):
        """Return prediction matrix.

        Parameters
        ----------
        dtype : numpy dtype object, optional
            data type to return

        order : str (default = 'C')
            data order. See :class:`numpy.asarray` for details.
        """
        if not hasattr(self, 'job'):
            raise ParallelProcessingError("Processor has been terminated: "
                                          "cannot retrieve final prediction "
                                          "array from cache.")
        if dtype is None:
            dtype = config.DTYPE

        if issparse(self.job.predict_out):
            return self.job.predict_out
        return np.asarray(self.job.predict_out, dtype=dtype, order=order)


###############################################################################
class ParallelEvaluation(BaseProcessor):

    """Parallel cross-validation engine.

    Parameters
    ----------
    caller : :class:`Evaluator`
        The ``Evaluator`` that instantiated the processor.
    """

    def __init__(self, *args, **kwargs):
        super(ParallelEvaluation, self).__init__(*args, **kwargs)

    def process(self, caller, case, X, y, path=None):
        """Fit estimators"""
        self._initialize(job='fit', X=X, y=y, path=path)
        check_initialized(self)

        # Use context manager to ensure same parallel job during entire process
        with Parallel(n_jobs=self.n_jobs,
                      temp_folder=self.job.dir,
                      max_nbytes=None,
                      mmap_mode='w+',
                      verbose=self.verbose,
                      backend=self.backend) as parallel:

            caller.indexer.fit(
                self.job.predict_in, self.job.y, self.job.job)

            caller(parallel, self.job.args, case)
