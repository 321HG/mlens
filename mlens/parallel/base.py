"""ML-Ensemble

:author: Sebastian Flennerhag
:copyright: 2017
:license: MIT

Base classes for parallel estimation
"""
from abc import abstractmethod
import numpy as np

from ._base_functions import mold_objects
from .. import config
from ..utils.exceptions import ParallelProcessingError
from ..externals.sklearn.base import BaseEstimator


class Group(BaseEstimator):

    """Group

    Lightweight class for pairing an estimator to a set of transformers and
    learners. Allows cloning.
    """

    def __init__(self, indexer, learners, transformers):
        learners, transformers = mold_objects(learners, transformers)

        # Enforce common indexer
        self.indexer = indexer
        for o in learners + transformers:
            o.set_indexer(self.indexer)

        self.learners = learners
        self.transformers = transformers

    def __iter__(self):
        for tr in self.transformers:
            yield tr
        for lr in self.learners:
            yield lr

    @property
    def __fitted__(self):
        """Fitted status"""
        return all([o.__fitted__ for o in self.learners + self.transformers])


class IndexMixin(object):

    """Indexer mixin

    Mixin for handling indexers.
    """

    @property
    def __indexer__(self):
        """Flag for existence of indexer"""
        return hasattr(self, 'indexer') or hasattr(self, 'indexers')

    def _check_indexer(self, indexer):
        """Check consistent indexer classes"""
        cls = indexer.__class__.__name__.lower()
        if 'index' not in cls:
            ValueError("Passed indexer does not appear to be valid indexer")

        lcls = [idx.__class__.__name__.lower() for idx in self._get_indexers()]
        if lcls:
            if 'blendindex' in lcls and cls != 'blendindex':
                raise ValueError(
                    "Instance has blendindex, but was passed full type")
            elif 'blendindex' not in lcls and cls == 'blendindex':
                raise ValueError(
                    "Instance has full type index, but was passed blendindex")

    def _get_indexers(self):
        """Return list of indexers"""
        if not self.__indexer__:
            raise AttributeError("No indexer or indexers attribute available")
        indexers = [getattr(self, 'indexer', None)]
        if None in indexers:
            indexers = getattr(self, 'indexers', [None])
        return indexers

    def _setup_0_index(self, X, y, job):
        indexers = self._get_indexers()
        for indexer in indexers:
            indexer.fit(X, y, job)


class OutputMixin(IndexMixin):

    """Output Mixin

    Mixin class for interfacing with ParallelProcessing when outputs are
    desired.

    .. Note::
       To use this mixin the instance inheriting it must set the
       ``feature_span`` attribute in ``__init__``.
    """

    @abstractmethod
    def set_output_columns(self, X, y, job, n_left_concats=0):
        """Set output columns for prediction array"""
        pass

    def _setup_3_output_columns(self, X, y, job, n_left_concats=0):
        """Set output columns for prediction array. Used during setup"""
        if not self.__no_output__:
            self.set_output_columns(X, y, job, n_left_concats)

    def shape(self, job):
        """Prediction array shape"""
        if not hasattr(self, 'feature_span'):
            raise ParallelProcessingError(
                "Instance dose not set the feature_span attribute "
                "in the constructor.")

        if not self.feature_span:
            raise ValueError("Columns not set. Call set_output_columns.")
        return self.size(job), self.feature_span[1]

    def size(self, attr):
        """Get size of dim 0"""
        if attr not in ['n_test_samples', 'n_samples']:
            attr = 'n_test_samples' if attr != 'predict' else 'n_samples'

        indexers = self._get_indexers()
        sizes = list()
        for indexer in indexers:
            sizes.append(getattr(indexer, attr))

        sizes = np.unique(sizes)
        if not sizes.shape[0] == 1:
            raise ValueError(
                "Inconsistent output sizes generated by indexers.\n"
                "All indexers need to generate same output size.\n"
                "Got sizes %r from indexers %r" % (sizes.tolist(), indexers))

        return sizes[0]


class ProbaMixin(object):

    """"Probability Mixin

    Mixin for probability features on objects
    interfacing with :class:`ParallelProcessing`

    .. Note::
       To use this mixin the instance inheriting it must set the ``proba``
       attribute in ``__init__``.
    """

    def _setup_2_multiplier(self, X, y, job=None):
        if self.proba and y is not None:
            self.classes_ = y

    def _get_multiplier(self, X, y, alt=1):
        if self.proba:
            multiplier = self.classes_
        else:
            multiplier = alt
        return multiplier

    @property
    def _predict_attr(self):
        return 'predict' if not self.proba else 'predict_proba'

    @property
    def classes_(self):
        """Prediction classes during proba"""
        return self._classes

    @classes_.setter
    def classes_(self, y):
        """Set classes given input y"""
        self._classes = np.unique(y).shape[0]


class AttributeMixin(object):

    """Base Estimator class

    Derived class from Scikit-learn's BaseEstimator. Protects parameters
    by forcing a re-run of ``__init__`` to ensure backend is updated.

    Child classes must set ``__initialized__ = 0`` on first line in
    ``__init__`` and ``__initialized__ = 1`` on last line in ``__init__``.

    Examples
    --------
    ::
        class Foo(_BaseEstimator):

            def __init__(self, bar):
                self.__initialized__ = 0  # Unblock __setattr__
                self.bar = bar
                self.__initialized__ = 1  # Protect __setattr__ wrt params chg
    """

    def __setattr__(self, name, value):
        reinit = False
        if getattr(self, '__initialized__', False):
            params = self.get_params(deep=False)
            if name in params:
                reinit = True
                params[name] = value
        if reinit:
            self.__init__(**params)
        else:
            super(AttributeMixin, self).__setattr__(name, value)


class BaseBackend(object):

    """Base class for parallel backend

    Implements default backend settings.
    """

    def __init__(self, backend=None, n_jobs=-1, dtype=None,
                 raise_on_exception=True):
        self.n_jobs = n_jobs
        self.dtype = dtype if dtype is not None else config.DTYPE
        self.backend = backend if backend is not None else config.BACKEND
        self.raise_on_exception = raise_on_exception


class BaseParallel(BaseBackend, BaseEstimator):

    """Base class for parallel objects

    Parameters
    ----------
    name : str
        name of instance. Should be unique.

    backend : str or object (default = 'threading')
        backend infrastructure to use during call to
        :class:`mlens.externals.joblib.Parallel`. See Joblib for further
        documentation. To set global backend, set ``mlens.config.BACKEND``.

    raise_on_exception : bool (default = True)
        whether to issue warnings on soft exceptions or raise error.
        Examples include lack of layers, bad inputs, and failed fit of an
        estimator in a layer. If set to ``False``, warnings are issued instead
        but estimation continues unless exception is fatal. Note that this
        can result in unexpected behavior unless the exception is anticipated.

    verbose : int or bool (default = False)
        level of verbosity.

    n_jobs : int (default = -1)
        Degree of concurrency in estimation. Set to -1 to maximize
        paralellization, while 1 runs on a single process (or thread
        equivalent). Cannot be overriden in the :attr:`add` method.

    dtype : obj (default = np.float32)
        data type to use, must be compatible with a numpy array dtype.
    """

    def __init__(self, name, *args, **kwargs):
        super(BaseParallel, self).__init__(*args, **kwargs)
        self.name = name
        self.__no_output__ = getattr(self, '__no_output__', False)

    def __iter__(self):
        """Iterator for process manager"""
        yield self

    @property
    @abstractmethod
    def __fitted__(self):
        """Fit status"""
        return

    def setup(self, X, y, job, skip=None, **kwargs):
        """Setup instance for estimation"""
        skip = ['_setup_%s' % s for s in skip] if skip else []
        funs = [f for f in dir(self)
                if f.startswith('_setup_') and f not in skip]

        for f in sorted(funs):
            func = getattr(self, f)
            args = func.__func__.__code__.co_varnames
            fargs = {k: v for k, v in kwargs.items() if k in args}
            func(X, y, job, **fargs)

    def get_params(self, deep=True):
        """Get learner parameters

        Parameters
        ----------
        deep : bool
            whether to return nested parameters
        """
        out = super(BaseParallel, self).get_params(deep=deep)
        for par_name in self._get_param_names():
            par = getattr(self, '_%s' % par_name, None)
            if par is None:
                par = getattr(self, par_name, None)
            if deep and hasattr(par, 'get_params'):
                for key, value in par.get_params(deep=True).items():
                    out['%s__%s' % (par_name, key)] = value
            out[par_name] = par

        for name in BaseBackend.__init__.__code__.co_varnames:
            if name not in ['self']:
                out[name] = getattr(self, name)
        return out
